---
title: "F.R.I.E.N.D.S Text Mining & Data Visualization"
subtitle: "Introduction to Tidy Text Package"
author: 
  name: "Ekrem Bayar"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: yes
    number_sections: yes
    code_folding: hide
    theme: spacelab
    highlight: tango
---

<center><img
src="https://s3.ap-south-1.amazonaws.com/hsdreams1/pins/2018/03/big/dc6897298d45ee9255cd71d403a38642.gif" style="width:100%;height:100%;">
</center>


# **PACKAGES & DATA** {.tabset .tabset-fade .tabset-pills}

## PACKAGES

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(DT)
library(png)
library(grid)
```

## DATA

```{r message=FALSE, warning=FALSE}
# Friends Logo
img <- readPNG("../input/friends/friends.png")
img <- rasterGrob(img, interpolate=TRUE)

afinn <- read.csv("../input/sentiment-lexicons-for-text-mining/afinn.csv", stringsAsFactors = FALSE)[-1]
nrc <- read.csv("../input/sentiment-lexicons-for-text-mining/nrc.csv", stringsAsFactors = FALSE)[-1]
bing <- read.csv("../input/sentiment-lexicons-for-text-mining/bing.csv", stringsAsFactors = FALSE)[-1]
loughran <- read.csv("../input/sentiment-lexicons-for-text-mining/loughran.csv", stringsAsFactors = FALSE)[-1]


# Text 
df <- read.csv("../input/friends-transcript/friends_quotes.csv", stringsAsFactors = FALSE, encoding = "UTF-8")
df %>% as_tibble()
```

```{r message=FALSE, warning=FALSE}
df %>% group_by(season) %>% summarise(episode_number = max(episode_number))
```

# **TIDY TEXT**

<center><img
src="https://i.pinimg.com/originals/bc/8e/cb/bc8ecbd124b1e51adcf3e64f5da90437.gif" style="width:100%;height:100%;">
</center>

<br>

```{r message=FALSE, warning=FALSE}
tidy_text <- df %>%
  # N-Gram
  unnest_tokens(word, quote) %>% 
  # Remove Stop Words
  anti_join(stop_words) %>%
  # Remove Character Names
  filter(!word %in% tolower(author))

# I removed some words
tidy_text <- tidy_text %>% 
  filter(!word %in% c("uhm", "it’s", "ll", "im", "don’t", "i’m", "that’s", "ve", "that’s","you’re",
                      "woah", "didn", "what're", "alright", "she’s", "we’re", "dont", "c'mere", "wouldn",
                      "isn","pbs", "can’t", "je", "youre", "doesn", "007", "haven", "whoah", "whaddya", 
                      "somethin", "yah", "uch", "i’ll","there’s", "won’t", "didn’t", "you’ll", "allright",
                      "yeah", "hey", "uh", "gonna", "umm","um", "y'know", "ah", "ohh", "wanna", "ya", "huh", "wow",
                      "whoa", "ooh")) %>% 
  mutate(word = str_remove_all(word, "'s")) 

tidy_text %>% as_tibble()
```

# **SENTIMENT LEXICONS** {.tabset .tabset-fade .tabset-pills}

- Bing
- NRC
- Afinn
- Loughran

## Bing Lexicon
```{r message=FALSE, warning=FALSE}
datatable(bing)
```

## NRC Lexicon
```{r message=FALSE, warning=FALSE}
datatable(nrc)
```

## Afinn Lexicon
```{r message=FALSE, warning=FALSE}
datatable(afinn)
```

## Loughran Lexicon
```{r message=FALSE, warning=FALSE}
datatable(loughran)
```


# **SENTIMENT ANALYSIS**

```{r message=FALSE, warning=FALSE}
tidy_bing <- tidy_text %>% inner_join(bing)
tidy_nrc <- tidy_text %>% inner_join(nrc)
tidy_afinn <- tidy_text %>% inner_join(afinn)
```


```{r fig.width=15, fig.height=10}
tidy_nrc %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  ggplot(aes(sentiment, fill = author))+
  geom_bar(show.legend = FALSE)+
  facet_wrap(author~.)+
  theme_dark()+
  theme(
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
  )+
  labs(fill = NULL, x = NULL, y = "Sentiment Frequency", title = "Sentiments of each characters by using nrc lexicon")+
  scale_fill_manual(values = c("#EA181E", "#00B4E8", "#FABE0F","#EA181E", "#00B4E8", "#FABE0F"))+
  annotation_custom(img,ymax = 4000, ymin = 2000, xmin = 1, xmax = 5)
```

  
```{r fig.width=15, fig.height=10}
tidy_bing %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  group_by(season, author) %>% 
  count(sentiment) %>%
  ungroup() %>%
  ggplot(aes(season, n, fill = sentiment)) +
  geom_col(position = "fill") +
  geom_text(aes(label = n), position = position_fill(0.5), color = "white")+
  coord_flip()+
  facet_wrap(author~.)+
  theme_dark()+
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
    )+
  scale_fill_manual(values = c("#EA181E", "#00B4E8"))+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10))+
  labs(y = NULL,  x = "Season", fill = NULL, title = "Negative-Positive Ratio in all seasons by using bing lexicon")
```

```{r message=FALSE, warning=FALSE, fig.width=15, fig.height=10}
df %>% 
  group_by(season) %>% 
  mutate(seq = row_number()) %>% 
  ungroup() %>% 
  unnest_tokens(word, quote) %>% 
  anti_join(stop_words) %>% 
  filter(!word %in% tolower(author)) %>% 
  inner_join(bing) %>% 
  count(season, index = seq %/% 50, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(index, sentiment, fill = factor(season))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(paste0("Season ",season)~., ncol = 2, scales = "free_x")+
  theme_dark()+
  theme(plot.title = element_text(hjust = 0.5, size = 20, face = "bold"))+
  labs(x = "Index", y = "Sentiment", title = "Negative-Positive Distribution in all seasons by using afinn lexicon")
```


```{r fig.width=15, fig.height=10}
all <- tidy_afinn %>% 
  mutate(Episode = factor(paste0("S",season,"-","E",episode_number))) %>% 
  group_by(Episode) %>% 
  summarise(total = sum(value), .groups = 'drop') %>% 
  ungroup %>% 
  mutate(Neg = if_else(total < 0, TRUE, FALSE))


ggplot()+
  geom_path(all, mapping = aes(Episode, total, group = 1), color = "#BA0E00")+
  geom_hline(mapping = aes(yintercept = 0), color = "#024D38")+
  theme_classic()+
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        plot.title = element_text(hjust = 0.5, color = "#EA181E", size = 20, face = "bold"))+
  geom_text((all %>% filter(Neg == TRUE)), mapping = aes(Episode, total-15, label = Episode), angle = 90, size = 3)+
  annotation_custom(img, ymin = 170, ymax = 220, xmin = 10, xmax = 60)+
  labs(title = "Total Sentiment Score each Episode with Afinn Lexicon", 
       y = "Total Sentiment Score")
```
  

```{r fig.width=15, fig.height=10}
tidy_afinn %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  group_by(season, author) %>% 
  summarise(total = sum(value), .groups = 'drop') %>% 
  ungroup() %>% 
  mutate(Neg = if_else(total < 0, TRUE, FALSE)) %>% 
  ggplot()+
  geom_path(aes(season, total, color = author), size = 1.2)+
  theme_minimal()+
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10))+
  scale_color_manual(values = c("#EA181E", "#00B4E8", "#FABE0F", "seagreen2", "orchid", "royalblue"))+
  labs(x = "Season", color = NULL, y = "Total Sentiment Score")+
  annotation_custom(img, ymin = 350, ymax = 400, xmin = 1, xmax = 4)
```
  
  
# **TF-IDF**

```{r message=FALSE, warning=FALSE, fig.width=15, fig.height=10}
tidy_text %>% 
  group_by(season) %>% 
  count(word) %>% 
  ungroup() %>% 
  bind_tf_idf(word, season, n) %>% 
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%  
  group_by(season) %>% 
  count(word) %>% 
  ungroup() %>% 
  bind_tf_idf(word, season, n) %>% 
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(season) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf, fill = factor(season))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "TF-IDF", "TF-IDF & Seasons") +
  facet_wrap(~season, ncol = 3, scales = "free") +
  coord_flip()+
  scale_fill_ordinal()+
  theme(plot.title = element_text(hjust = 0.5, size = 20, face = "bold"))
  
```


# **TOPIC MODELLING** {.tabset .tabset-fade .tabset-pills}
```{r}
# Document-Term Matrix
dtm <- tidy_text %>% 
  select(season, word) %>% 
  group_by(season, word) %>% 
  count() %>% 
  cast_dtm(season, word, n)

dtm

# set a seed so that the output of the model is predictable
lda <- LDA(dtm, k = 10, control = list(seed = 1234))
lda
```

## Word-Topic Probabilities 
```{r message=FALSE, warning=FALSE, fig.width=15, fig.height=10}
topics <- tidy(lda, matrix = "beta")

top_terms <- topics %>%group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)



top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()+
  labs(title = "Word-Topic Probabilities")+
  theme(plot.title = element_text(hjust = 0.5, size = 20, face = "bold"))
```


## Document-Topic Probabilities
```{r fig.width=15, fig.height=10}
documents <- tidy(lda, matrix = "gamma")

documents %>% 
  ggplot(aes(document, gamma, fill = factor(topic)))+
  geom_col(position = "fill")+
  labs(x = "Season", fill = "Topic", y = "Gamma", title = "Document-Topic Probabilities")+
  scale_fill_ordinal()+
  theme(
    legend.position = "top",
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
  )
```

# **POSTMODERN JUKEBOX**

<center><iframe width="560" height="315" src="https://www.youtube.com/embed/3q7ExHaKt2M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>

# **TIDYTEXT PACKAGE**

Julia Silge and David Robinson wrote [Text Mining with R](https://www.tidytextmining.com/) book. 

"This book serves as an introduction of text mining using the tidytext package and other tidy tools in R. The functions provided by the tidytext package are relatively simple; what is important are the possible applications. Thus, this book provides compelling examples of real text mining problems."
  
<center><img
src="https://media1.giphy.com/media/H1vjPkqdL7liL6O7Bj/giphy.gif" style="width:100%;height:100%;">
</center>